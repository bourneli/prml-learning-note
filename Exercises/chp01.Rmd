---
title: "PRML-Exercises-Chapter-01"
author: "bourneli"
output: html_document
---
# 1.1 (*)  

计算损失函数的最优解，
$E(w)=\frac{1}{2}\sum_{n=1}^{N}(\sum_{j=0}^{M}w_jx_n^j-t_n)^2$

偏导数为0，
$$\frac{\partial E(w)}{w_i}
  = \sum_{n=1}^{N}(\sum_{j=0}^{M}w_jx_n^j-t_n)x_n^i
  = 0
$$

最后化简，得到最终答案,注意左边两个$\Sigma$可以交换位置的。

$$
  \sum_{n=1}^{N}\sum_{j=0}^{M}w_jx_n^{i+j} 
  = \sum_{j=0}^{M}w_j\sum_{n=1}^{N}x_n^{i+j} 
  =\sum_{j=0}^{M}w_jA_{ij} = \sum_{n=1}^{N}t_nx_n^i=T_i
$$

# 1.2 (*)



与1.1题类似，考察了在L2正规化情况下的最有计算方案，此时损失函数为

$$
  \widetilde{E}(w)=\frac{1}{2}\sum_{n=1}^{N}(\sum_{j=0}^{M}w_jx_n^j-t_n)^2
   + \frac{\lambda}{2}\|w\|^2
$$


此时引入了一个参数$\lambda$，人工输入。仍然求偏导，

$$\frac{\partial \widetilde{E}(w)}{w_i}
  = \sum_{n=1}^{N}(\sum_{j=0}^{M}w_jx_n^j-t_n)x_n^i + \lambda w_i
  = 0
$$

其他类似，这里省略。

# 1.3 (*)

苹果的概率列举所有可能，使用条件概率即可

$$
  p(apple) = p(red)p(apple|red) + p(blue)p(apple|blue) + p(green)p(apple|green) \\
           = 0.2 \times 0.3 + 0.2 \times 0.5 + 0.6 \times 0.3 = 0.34
$$


在得到橘子的情况下，计算从绿色罐中得到概率不太好计算，但是可以用贝叶斯计算这个后置概率。这里要强调一点，**前置概率**，即任何一个水果从绿色罐中得到的概率是0.6。

$$
  p(green|orange) = \frac{p(orange|green)p(green)}{p(orange)} \\
                  = \frac{0.3 \times 0.6}{0.2\times 0.4 + 0.2 \times 0.5 + 0.6 \times 0.3}  = 0.5
$$

可以发现，这个后验概率比先验概率要小。因为橘子在绿色的概率比较小，拖了绿色先验概率的后腿。


# 1.5 (*)

考察期望计算符号性质

$$
  \begin{align}
  var[f(x)] &= E[(f(x)-E[f(x)])^2] \\
            &= E[(f(x))^2+E[f(x)]^2-2f(x)E[f(x)]] \\
            &= E[(f(x))^2]+E[f(x)]^2-2E[f(x)]E[f(x)] \\
            &= E[(f(x))^2]-E[f(x)]^2
  \end{align}
$$

# 1.6 (*)

仍然考察期望符号性质，协方差按性质展开,x,y独立有$E(xy)=E(x)E(y)$

$$
  \begin{align}
    cov(x,y) &= E[(x-\bar{x}(y-\bar{y})] \\
            &= E[xy]- 2\bar{x}\bar{y}+\bar{x}\bar{y} \\
            &= E[x]E[y] - \bar{x}\bar{y} = 0
  \end{align}
$$


# 1.11 (*)

考察使用最大释然法求解正太分布的过程。目标函数如下

$$
  J(\mu, \sigma^2) = \ln{p(X|\mu,\sigma^2)} 
  = -\frac{1}{2\sigma^2}\sum_{n=1}^{N}(x_n-\mu)^2-\frac{N}{2}\ln{\sigma^2}-\frac{N}{2}\ln{(2\pi)}
$$


目标函数对$\mu$求偏导，并且等于0
$$
  \frac{\partial{J(\mu,\sigma^2)}}{\partial{\mu}}
  = \frac{1}{\sigma^2}\sum_{n=1}^{N}{(x_n-\mu)} = 0
  \Rightarrow \mu=\frac{1}{N}\sum_{n=1}^{N}x_n
$$



目标函数对$\sigma^2$求偏导(**不是$\sigma$**)，并且等于0，

$$
   \frac{\partial{J(\mu,\sigma^2)}}{\partial{\sigma^2}} 
   = \frac{1}{2(\sigma^2)^2}\sum_{n=1}^{N}(x_n-\mu)^2-\frac{N}{2\sigma^2} = 0
   \Rightarrow \sigma^2 = \frac{1}{N}\sum_{n=1}^{N}(x_n-\mu)^2
$$


# 1.12 (**)

这个问题解释了为什么用$\frac{\frac{1}{N}\sum_{n=1}^{N}(x_n-\mu)^2}{n-1}$来评估正太分布的方差，其中$\mu$是样本均值。

已知下面等式

$$
  E[x] = \int_{-\infty}^{+\infty}{N(x|\mu, \sigma^2)}x dx = \mu  \qquad (1.49) \\
  E[x^2] = \int_{\infty}^{+\infty}{N(x|\mu, \sigma^2)}x^2 dx = \mu^2 + \sigma^2 (1.50) 
$$

当$m=n$,直接使用公式(1.50)，有$E[x_n^2]=\mu^2+\sigma^2$

当$m \ne n$,由于$x_m$,$x_n$独立,所以$E[x_mx_n]=E[x_n]E[x_m]=\mu^2$

证毕。下面证明(1.57)和(1.58)

$$
  E[\mu_{ML}] = \mu \qquad (1.57) \\
  E[\sigma^2_{ML}] = \left(\frac{N-1}{N}\right)\sigma^2 \qquad (1.58)
$$


证明(1.57)

$$
  E[\mu_{ML}] = E\left[\frac{1}{N}\sum_{n=1}^Nx_n \right] 
  = \frac{1}{N}\sum_{n=1}^NE[x_n] 
  = \frac{1}{N}\sum_{n=1}^N\mu
  = \mu
$$


证明(1.58)

$$
  \begin{align}
    E[\sigma^2_{ML}] &= E\left[\frac{1}{N}\sum_{n=1}^{N}(x_n-\frac{1}{N}\sum_{m=1}^Nx_m)^2 \right] \\
    &= \frac{1}{N}\sum_{n=1}^N\left( E[x_n^2] 
      + E\left[(\frac{1}{N}\sum_{m=1}^Nx_m)^2\right]
      -2E\left[x_n\frac{1}{N}\sum_{m=1}^{N}x_m\right]  \right) \\
    &= \frac{1}{N}\sum_{n=1}^N\left(\mu^2+\sigma^2
        + \frac{1}{N^2}E\left[\sum_{i=1}^N\sum_{j=1}^Nx_ix_j\right] 
        - 2E\left[\frac{x_n}{N}\sum_{m=1}^Nx_m\right]  \right) \\
    &= \mu^2 + \sigma^2 + \frac{1}{N^2}(N\sigma^2+(N\mu)^2) 
      - \frac{2}{N^2}E\left[\sum_{n=1}^{N}\sum_{m=1}^{N}x_nx_m\right] \\
    &= 2\mu^2+(1+\frac{1}{N})\sigma^2-\frac{2}{N^2}((N\mu)^2+N\sigma^2) \\
    &= \frac{N-1}{N}\sigma^2
  \end{align}
$$

所以用$\sigma^2=\frac{N}{N-1}\sigma_{ML}^2 = \frac{1}{N-1}\sum_{n=1}^N(x_n-\mu_{ML})^2$估计方差！

# 1.13(*)

此题说明了，如果已知真实均值$\mu$，然后直接用$\sigma_{ML}^2=\frac{1}{N}\sum_{n=1}^M{(x_n-\mu)^2}$得到的方差是无偏的，不需要将分母换成$N-1$。直接计算上述方差的期望，可以非常容易得到结果，这里省略。

正太分布的总结

* 样本均值可以直接估计分布均值
* 样本方差估计分布方差是有偏的，需要修正，修正的方法是将分母换成$N-1$
* 如果已知真实均值，那么样本方差估计分布方差是无偏的

# 1.17(**)

使用分部积分，证明$\Gamma$函数与阶乘的关系。

递归关系，

$$
  \begin{align}
  \Gamma(x+1) &= \int_0^{+\infty}{u^xe^{-u}du} \\
              &= -\int_0^{+\infty}u^x d(e^{-u}) \\
              &= -\left[u^xe^{-u}|_0^{+\infty}-\int_0^{+\infty}e^{-u}d(u^x) \right] \\
              &= \int_0^{+\infty}xu^{x-1}e^{-u}dx \\
              &= x\Gamma(x)
  \end{align}
$$

初始化条件，


$$
  \Gamma(1) = \int_0^{+\infty}e^{-u}du 
            = -\inf_0^{+\infty}e^{-u}d(-u)
            = - [e^{-u}]|^{+\infty}_0 = 1
$$


根据上面两个条件，在x为整数时，很容易得到$\Gamma(x+1)=x!$

# 1.21 (**)

对于二元分类，通过某算法，找到了范围$R_1$和$R_1$，使得

* $x \in R_1, p(C_1|x) \ge P(C_2|x)$
* $x \in R_2, P(C_2|x) \ge P(C_1|x)$

这样可以使得p(mistake)最小，（见教材1.5.1）。所以，据此，可以推导p(mistake)范围如下

$$
  \begin{align}
    p(mistake) &= \int_{R_1} p(C_2, x) dx + \int_{R_2} p(C_1, x) dx \\
               &= \int_{R_1} p(C_2|x)p(x) dx + \int_{R_2} p(C_1|x)p(x) dx \\
               &\le \int_{R_1} (p(C_1|x)p(C_2|x))^{\frac{1}{2}}p(x) dx 
                     + \int_{R_2} (p(C_1|x)P(C_2|x))^{\frac{1}{2}} p(x) dx \\
               &= \int (p(C_1|x)p(C_2|x))^{\frac{1}{2}}p(x) dx \\
               &= \int (p(C_1|x)p(x)p(C_2|x)p(x))^{\frac{1}{2}} dx \\
               &= \int \left(p(C_1,x)p(C_2,x)\right)^{\frac{1}{2}} dx \\
  \end{align}
$$

证毕。

# 1.22 (*)

根据条件，等价于每个错误的损失权重相等，与不使用损失矩阵的情况等价。 推导方法如下：

$$
  \sum_k(1-I_{kj})P(C_k|x) = \sum_kP(C_k|x) - \sum_kI_{kj}p(C_k|x)
  = 1- p(C_j|x)
$$

最小化上面公式，等价于最大化$p(C_j|x)$。


# 1.29 (*)

考察凸函数与Jensen不等式的活用。Jenen不等式，定义在**凸函数**上，有

$$
  E(\sum_{i=1}^M \lambda_i x_i) \le \sum_{i=1}^M \lambda_i f(x_i)
$$

如果$f(x)$是凹函数，那么Jessen不等式需要反号，如下

$$
  E(\sum_{i=1}^M \lambda_i x_i) \ge \sum_{i=1}^M \lambda_i f(x_i)
$$

对于x熵$E[x]=-\sum_{i=1}^M p(x_i) \ln{p(x_i)}= \sum_{i=1}^M p(x_i) \ln \frac{1}{p(x_i)}$,其中$\ln{x}$是**凹函数**，所以有


$$
  H[x]  = \sum_{i=1}^M p(x_i) \ln \frac{1}{p(x_i)} \le 
        \ln{\sum_{i=1}^M p(x_i) \frac{1}{p(x_i)}} 
        = \ln{M}
$$

# 1.30 (*)

此题虽然是考察KL散度，但是主要是考察正太分布的相关二次，一次积分。KL散度用于度量理论分布与模拟分布的差异。首席，会议一下需要用到的正太分布积分

$$
  \int_{-\infty}^{+\infty} N(x|\mu,\sigma^2) dx = 1 \qquad (1) \\
  \int_{-\infty}^{+\infty} N(x|\mu,\sigma^2)x dx = \mu \qquad (2) \\
  \int_{-\infty}^{+\infty} N(x|\mu,\sigma^2)x^2 dx = \mu^2+\sigma^2 \qquad (3) \\
$$

回到本题，将两个分布代入到KL散度中，得到如下

$$
  \begin{align}
  KL(p\|q) &= -\int_{-\infty}^{+\infty}p(x)\ln{\frac{q(x)}{p(x)}} dx \\
           &= -\int_{-\infty}^{+\infty}N(x|\mu,\sigma^2)\left(\ln{N(x|m,s^2)} - \ln{N(x|\mu,\sigma^2)}\right) dx \\
           &= -\int_{-\infty}^{+\infty}N(x|\mu,\sigma^2)\left(
                    -\ln{\sigma\sqrt{2\pi}}-\frac{(x-\mu)^2}{2\sigma^2} + \ln{s\sqrt{2\pi} + \frac{(x-m)^2}{2s^2}}  \right) dx \\
           &= -\int_{-\infty}^{+\infty}N(x|\mu,\sigma^2)
            \left(
              (\frac{1}{2s^2}-\frac{1}{2\sigma^2})x^2 +     
              (\frac{\mu}{\sigma^2}-\frac{m}{s^2})x +
               \ln{\frac{s}{\sigma} + \frac{m^2}{2s^2} - \frac{\mu^2}{2\sigma^2}} 
            \right) dx     \\
          &= \frac{1}{2}(\ln{\frac{s^2}{\sigma^2}+\frac{\sigma^2+(m-\mu)^2}{s^2}}-1)   
  \end{align}
$$

# 1.31 (**)

此题考查了熵与互信息的关系。由联合分布的熵，可得到（证明教材没有，参考[wiki](https://zh.wikipedia.org/wiki/%E6%9D%A1%E4%BB%B6%E7%86%B5)）$H[x,y] = H[x|y] + H[x]$。又由于互信息$I(x,y)=H[y]-H[y|x] \ge 0$，所以有$H[x,y] \le H[x] + H[y]$。当x,y独立时，互信息$I(x,y)=0$,此时等式相等。

还有一个直观的解释，熵是信息量的度量。如果x,y有相关性，那么知道了x，可以知道部分y，那么$H[y|x]$包含的信息就是部分$H[y]$的信息。如果x,y独立，那么知道x不会得到任何y的信息，那么$H[y|x]$包含的信息就是全部$H[y]$。


# 1.33 (**)

将条件熵展开(参考[这里](https://zh.wikipedia.org/wiki/%E6%9D%A1%E4%BB%B6%E7%86%B5))
$H[y|x] = \sigma_i\sigma_j p(x_j) p(y_i|x_j)\ln{p(y_i|x_j)} = 0$。

所以每个元素都等于0， 且$p(x_j) > 0$,那么$p(y_i|x_j)\ln{p(y_i|x_j)} = 0$。因为$p\ln{p}$只有当p=0或1时为0。$p(y_i|x_j) = 0 或 1$，并且所有$p(y_i|x_j)$之和为1，所以必有一个$p(y_i|x_j)=1$其他的等于0.

# 1.35 (*)

利用泛函分析，得到的最优函数，代入熵计算方法，计算过程如下

$$
  \begin{align}
  H[x] &= -\int_{-\infty}^{+\infty} N(x|\mu,\sigma^2)
          \ln{\left( \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}     \right)} dx \\
       &= -\int_{-\infty}^{+\infty} N(x|\mu,\sigma^2)
          \left( \ln{\frac{1}{\sqrt{2\pi}\sigma}} -\frac{(x-\mu)^2}{2\sigma^2} \right) dx \\ 
       &= -\ln{\frac{1}{\sqrt{2\pi}\sigma}} + \int_{-\infty}^{+\infty}{\frac{x^2+\mu^2-2\mu x}{2\sigma^2}N(x|\mu,\sigma^2)} dx \\
       &= \ln{\sqrt{2\pi}\sigma} + \frac{1}{2\sigma^2}(\mu^2+\sigma^2+\mu^2-2\mu^2) \\
       &= \frac{1}{2}(1+ln{(2\pi\sigma^2)})
  \end{align}
$$

最优熵，与均值无关，只与方差有关。

# 1.37 (*)

只需理解下面计算方法，即可很容易求证

$$
  \begin{align}
    H[x] &= -\int_{-\infty}^{+infty} p(x) \ln{p(x)} dx \\
         &= -\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} p(x,y) dy \ln{p(x)} dx \\
         &= -\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} p(x,y) \ln{p(x)} dy dx \\
  \end{align}
$$

可以从离散概率联想，$p(X=x) = \sum_{i=1}^{N}  p(X=x,Y=i)$

 
 
# 1.38 (*)

使用数学归纳法，当M=2时，由条件易得。

假设当$M=K$时成立，即$f(\sum_{i=1}^K {\lambda_i x_i}) \le \sum_{i=1}^K \lambda_i f(x_i)$。

当$M=K+1$时，

$$
  \begin{align}
    f(\sum_{i=1}^{K+1} {\lambda_i x_i}) &= f\left(\sum_{i=1}^{K} ({\lambda_i x_i}) + \lambda_{K+1} x_{K+1} \right) \\
      &= f\left((1-\lambda_{K+1})\sum_{i=1}^{K} ({\frac{\lambda_i}{1-\lambda_{K+1}} x_i}) + \lambda_{K+1} x_{K+1} \right) \\
      &\le (1-\lambda_{K+1})f\left(\sum_{i=1}^{K} {\frac{\lambda_i}{1-\lambda_{K+1}} x_i} \right) + \lambda_{K+1}f(x_{K+1}) \qquad (1) \\
      &\le (1-\lambda_{K+1}) \frac{\lambda_i}{1-\lambda_{K+1}} \sum_{i=1}^K f(x_i)    + \lambda_{K+1}f(x_{K+1}) \qquad (2) \\
      &= \sum_{i=1}^{K+1} \lambda_i f(x_i)
  \end{align}
$$

证毕，其中(1)通过M=2时变化得到，(2)通过假设M=k时得到。
 
 
# 1.39 (***)

$H[x] = -(\frac{2}{3}\ln{\frac{2}{3} + \frac{1}{3}\ln{\frac{1}{3}}}) = 0.6365142$

$H[y] = -(\frac{1}{3}\ln{\frac{1}{3}} + \frac{2}{3}\ln{\frac{2}{3}}) = 0.6365142$

$H[x,y] = -(\frac{1}{3}\ln{\frac{1}{3}} + \frac{1}{3}\ln{\frac{1}{3}}  + \frac{1}{3}\ln{\frac{1}{3}}) =  1.098612$ x,y并集信息量

$H[x|y] = H[x,y] - H[y] = 0.4620978$  x中剔除交集部分信息量

$H[y|x] = H[x,y] - H[x] = 0.4620978$ y中剔除x部分信息量

$I[x,y] = H[x] - H[x|y] = 0.174416$ x,y交集信息量，如果独立，那么为0

# 1.40 (*)

$\ln$是凹函数，且是单调递增函数，所以利用1.115不等号需要反向，即$f(\sum_{i=1}^M {\lambda_i x_i}) \ge \sum_{i=1}^M \lambda_i f(x_i)$。将$\ln$代入此不等式，有


$$
  \ln{\sum^M \lambda_i x_i} \ge \sum^M\lambda_i\ln{x_i} \\
  \sum^M\lambda_i x_i \ge \prod^Mx_i^{\lambda_i} \qquad 根据单调递增  \\ 
 \frac{1}{M}\sum^M x_i \ge \left(\prod^Mx_i \right)^{\frac{1}{M}}  \qquad 令 \lambda_i = \frac{1}{M},证毕
$$

# 1.41 (*)

信息计算，原理类似集合计算，将微积分融入其中，感觉触类旁通。直接根据定义计算，逐步化简。

$$
  \begin{align}
  I[x,y] &= -\int\int p(x,y) \ln\left( {\frac{p(x)p(y)}{p(x,y)}} \right) dx dy \\
         &= -\int\int p(x,y) \ln\left( p(x)p(y) \right) dx dy +\int\int p(x,y) \ln\left( p(x,y) \right) dx dy \\
         &=  -\int\int p(x,y) \ln {p(x)} dx dy -\int\int p(x,y) \ln {p(y)} dx dy   - H[x,y] \\
         &=  -\int \left(\int p(x,y) dy \right) \ln {p(x)} dx -\int \left(\int p(x,y)dx\right) \ln {p(y)}  dy   - H[x,y] \\
         &=  -\int p(x) \ln{p(x)} dx -\int p(y) \ln{p(y)}  dy   - H[x,y] \\
         &= H[x] + H[y] - H[x,y] \\
         &= H[x,y] - H[y|x] + H[y] \\
         &= H[y] - H[y|x] \\
  \end{align} 
$$


